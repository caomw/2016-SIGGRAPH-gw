% !TEX root = ../gw.tex

\section{Discussion and Conclusion}

%\gabriel{I did not really understood the point in the following sentence, why is "stability" related to "renewed interest" ? Maybe I just misinterpreted "commonality" though. }
%Our discussion finds commonality with recent renewed interest in nonconvex optimization in graphics, machine learning, and other computational disciplines. 
%\justin{agreed.  it was a poorly written pointless sentence :-) }

%\gabriel{I am not so convinced by the argument made by the following sentence, that entropy is the key to make the method avoid local minimum. Is it true ? It does not seems to be validated by experiments in this article. Figure 1 even suggest the opposite conclusion! Large $\alpha$ leads to bad results ... } \justin{Your definition of "bad" and mine apparently disagree :-).  Expanded a bit to explain what I mean.  By the way, even in low alpha the regularizer is what's enabling us to get OK maps with such a simple algorithm.  So, it's all good!}
%\suv{Is it really ok to call it "variant of projected gradient descent"? or should we call it a variant of mirror descent (simply because it uses KL), or "exponentiated gradient descent"?}

Empirically, it appears our entropic regularizer transforms a quadratic matching energy and constraint landscape---well-known to be difficult to navigate---to a smooth one whose basins can be reached via a variant of projected gradient descent.   In this sense, despite the presence of multiple local optima for some mapping tasks, it is likely outside the class of ``scary'' optimization objectives avoided in tractable numerical tools~\cite{sun-2015} since these local optima all represent meaningful correspondences.

Our results suggest many potential extensions and challenging avenues for future research.  Principally, what remains is extension to \emph{partial} matching, e.g.\ taking part of a surface to a full target or vice versa.  An obvious approach is to drop either prescribed row or column sums for $\G$, but this leads to a larger space in which \GWa appears to have more local optima; this is reasonable in that there often exist many ways to align a patch of a domain with a larger space.  Application-specific regularization may help in this case.  An additional extension might be to improve the efficiency of our technique by alleviating the need for distance matrices on structured domains, similar to~\cite{solomon-2015}.

While a theme of this work is that regularization can make challenging matching problems tractable, the longest-standing challenge is to optimize GW and related objectives without smoothing.  General quadratic matching is NP-hard, but algorithms for minimizing quadratic objectives derived from smooth geometric domains may have greater hope of succeeding.  Recent progress for tasks in graphics including~\cite{chen-2015,wei-2015} suggests that there is much to do in this direction.

There is a formidable spectrum of tools for geometric correspondence.  Methods tuned to individual domain classes are well-explored for detailed, application-specific analysis.  Contrastingly, regularized GW optimization is a widely-applicable, straightforward, and easily-extended tool for rough correspondence given minimal guidance or assumptions.  Our examples show how an identical short piece of code can be used to understand shapes, surfaces, image collections, graphs, icons, and so on.  This practical contribution is paired with an unconditional convergence guarantee, a reasonable compromise between slow globally optimal and fast heuristic approaches.  As it stands, metric alignment via entropic GW distances provides an easy, stable approach to soft correspondence suitable across a wide collection of matching tasks.  